{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['clf']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data is 10% training data\n",
      "Ready for ML\n"
     ]
    }
   ],
   "source": [
    "#ETL of data frame, separation of training and testing data\n",
    "\n",
    "# Importing libraries\n",
    "\n",
    "import PySide\n",
    "\n",
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "#from skll import kappa\n",
    "\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "s = [\"Product_Info_1, Product_Info_2, Product_Info_3, Product_Info_5, Product_Info_6, Product_Info_7, Employment_Info_2, Employment_Info_3, Employment_Info_5, InsuredInfo_1, InsuredInfo_2, InsuredInfo_3, InsuredInfo_4, InsuredInfo_5, InsuredInfo_6, InsuredInfo_7, Insurance_History_1, Insurance_History_2, Insurance_History_3, Insurance_History_4, Insurance_History_7, Insurance_History_8, Insurance_History_9, Family_Hist_1, Medical_History_2, Medical_History_3, Medical_History_4, Medical_History_5, Medical_History_6, Medical_History_7, Medical_History_8, Medical_History_9, Medical_History_11, Medical_History_12, Medical_History_13, Medical_History_14, Medical_History_16, Medical_History_17, Medical_History_18, Medical_History_19, Medical_History_20, Medical_History_21, Medical_History_22, Medical_History_23, Medical_History_25, Medical_History_26, Medical_History_27, Medical_History_28, Medical_History_29, Medical_History_30, Medical_History_31, Medical_History_33, Medical_History_34, Medical_History_35, Medical_History_36, Medical_History_37, Medical_History_38, Medical_History_39, Medical_History_40, Medical_History_41\",\n",
    "    \"Product_Info_4, Ins_Age, Ht, Wt, BMI, Employment_Info_1, Employment_Info_4, Employment_Info_6, Insurance_History_5, Family_Hist_2, Family_Hist_3, Family_Hist_4, Family_Hist_5\",\n",
    "     \"Medical_History_1, Medical_History_10, Medical_History_15, Medical_History_24, Medical_History_32\"]\n",
    " \n",
    "\n",
    "varTypes = dict()\n",
    "\n",
    "\n",
    "varTypes['categorical'] = s[0].split(', ')\n",
    "varTypes['continuous'] = s[1].split(', ')\n",
    "varTypes['discrete'] = s[2].split(', ')\n",
    "varTypes['dummy'] = [\"Medical_Keyword_\"+str(i) for i in range(1,49)]\n",
    "\n",
    "\n",
    "#Import training data \n",
    "d_raw = pd.read_csv('prud_files/train.csv')\n",
    "d = d_raw.copy()\n",
    "\n",
    "\n",
    "# Get all the columns that have NaNs\n",
    "a = pd.isnull(d).sum()\n",
    "nullColumns = a[a>0].index.values\n",
    "\n",
    "#Determine the min and max values for the NaN columns\n",
    "a = pd.DataFrame(d, columns=nullColumns).describe()\n",
    "\n",
    "# Convert all NaNs to -1 and sum up all medical keywords across columns\n",
    "df = d.fillna(-1)\n",
    "b = pd.DataFrame(df[varTypes[\"dummy\"]].sum(axis=1), columns=[\"Medical_Keyword_Sum\"])\n",
    "df= pd.concat([df,b], axis=1, join='outer')\n",
    "\n",
    "\n",
    "#Turn split train to test on or off.  \n",
    "#If on, 10% of the dataset is used for feature training\n",
    "#If off, training set is loaded from file\n",
    "\n",
    "splitTrainToTest = 1\n",
    "\n",
    "if(splitTrainToTest):\n",
    "    \n",
    "    d_gb = df.groupby(\"Response\")\n",
    "    \n",
    "    #Partial data set to train\n",
    "    df_train = pd.DataFrame()\n",
    "    \n",
    "    #Partial data set to test\n",
    "    df_test = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for name, group in d_gb:\n",
    "        \n",
    "        test_g = group[:len(group)/100]\n",
    "        train_g = group[len(group)/100:]\n",
    "        df_test = pd.concat([df_test, test_g], axis=0, join='outer')\n",
    "        df_train = pd.concat([df_train, train_g], axis=0, join='outer')\n",
    "        \n",
    "    print \"test data is 10% training data\"\n",
    "    \n",
    "else:\n",
    "    d_test = pd.read_csv('prud_files/test.csv')\n",
    "    df_test = d_test.fillna(-1)\n",
    "    b = pd.DataFrame(df[varTypes[\"dummy\"]].sum(axis=1), columns=[\"Medical_Keyword_Sum\"])\n",
    "    df_test= pd.concat([df_test,b], axis=1, join='outer')\n",
    "    print \"test data is prud_files/test.csv\"\n",
    "    \n",
    "\n",
    "## Extract key columns for normalization\n",
    "\n",
    "df_train_n = df_train.copy()\n",
    "df_test_n = df_test.copy()\n",
    "\n",
    "#Get all the Product Info 2 categories\n",
    "\n",
    "a = pd.get_dummies(df[\"Product_Info_2\"]).columns.tolist()\n",
    "norm_PI2_dict = dict()\n",
    "\n",
    "#Create an enumerated dictionary of Product Info 2 categories\n",
    "\n",
    "i=1\n",
    "for c in a:\n",
    "    norm_PI2_dict.update({c:i})\n",
    "    i+=1 \n",
    "\n",
    "df_train_n = df_train_n.replace(to_replace={'Product_Info_2':norm_PI2_dict})\n",
    "df_test_n = df_test_n.replace(to_replace={'Product_Info_2':norm_PI2_dict})\n",
    "\n",
    "# normalizes a single dataframe column and returns the result\n",
    "\n",
    "def normalize_df(d):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x = d.values.astype(np.float)\n",
    "    return min_max_scaler.fit_transform(x)\n",
    "\n",
    "\n",
    "#Normalize relevant columns\n",
    "\n",
    "df_train_n = df_train_n[[\"Response\"]+varTypes[\"categorical\"]+varTypes[\"discrete\"]]\n",
    "df_test_n = df_test_n[[\"Response\"]+varTypes[\"categorical\"]+varTypes[\"discrete\"]]\n",
    "\n",
    "for col in df_train_n:\n",
    "    df_train_n[col] = normalize_df(df_train_n[col])\n",
    "for col in df_test_n:\n",
    "    df_test_n[col] = normalize_df(df_test_n[col])\n",
    "#Combine cells together\n",
    "\n",
    "\n",
    "df_train_n = pd.concat([pd.DataFrame(df_train.Id),df_train_n,df_train[varTypes['continuous']],pd.DataFrame(df_train.Medical_Keyword_Sum)], axis=1, join='outer')\n",
    "\n",
    "df_test_n = pd.concat([pd.DataFrame(df_test.Id),df_test_n,df_test[varTypes['continuous']],pd.DataFrame(df_test.Medical_Keyword_Sum)], axis=1, join='outer')\n",
    "\n",
    "print \"Ready for ML\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from six import string_types\n",
    "from six.moves import xrange as range\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, SCORERS\n",
    "\n",
    "\n",
    "### Imported from skll package.  http://skll.readthedocs.org/en/latest/_modules/skll/metrics.html\n",
    "\n",
    "def kappa(y_true, y_pred, weights=None, allow_off_by_one=False):\n",
    "    \"\"\"\n",
    "    Calculates the kappa inter-rater agreement between two the gold standard\n",
    "    and the predicted ratings. Potential values range from -1 (representing\n",
    "    complete disagreement) to 1 (representing complete agreement).  A kappa\n",
    "    value of 0 is expected if all agreement is due to chance.\n",
    "\n",
    "    In the course of calculating kappa, all items in `y_true` and `y_pred` will\n",
    "    first be converted to floats and then rounded to integers.\n",
    "\n",
    "    It is assumed that y_true and y_pred contain the complete range of possible\n",
    "    ratings.\n",
    "\n",
    "    This function contains a combination of code from yorchopolis's kappa-stats\n",
    "    and Ben Hamner's Metrics projects on Github.\n",
    "\n",
    "    :param y_true: The true/actual/gold labels for the data.\n",
    "    :type y_true: array-like of float\n",
    "    :param y_pred: The predicted/observed labels for the data.\n",
    "    :type y_pred: array-like of float\n",
    "    :param weights: Specifies the weight matrix for the calculation.\n",
    "                    Options are:\n",
    "\n",
    "                        -  None = unweighted-kappa\n",
    "                        -  'quadratic' = quadratic-weighted kappa\n",
    "                        -  'linear' = linear-weighted kappa\n",
    "                        -  two-dimensional numpy array = a custom matrix of\n",
    "                           weights. Each weight corresponds to the\n",
    "                           :math:`w_{ij}` values in the wikipedia description\n",
    "                           of how to calculate weighted Cohen's kappa.\n",
    "\n",
    "    :type weights: str or numpy array\n",
    "    :param allow_off_by_one: If true, ratings that are off by one are counted as\n",
    "                             equal, and all other differences are reduced by\n",
    "                             one. For example, 1 and 2 will be considered to be\n",
    "                             equal, whereas 1 and 3 will have a difference of 1\n",
    "                             for when building the weights matrix.\n",
    "    :type allow_off_by_one: bool\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Ensure that the lists are both the same length\n",
    "    assert(len(y_true) == len(y_pred))\n",
    "\n",
    "    # This rather crazy looking typecast is intended to work as follows:\n",
    "    # If an input is an int, the operations will have no effect.\n",
    "    # If it is a float, it will be rounded and then converted to an int\n",
    "    # because the ml_metrics package requires ints.\n",
    "    # If it is a str like \"1\", then it will be converted to a (rounded) int.\n",
    "    # If it is a str that can't be typecast, then the user is\n",
    "    # given a hopefully useful error message.\n",
    "    # Note: numpy and python 3.3 use bankers' rounding.\n",
    "    try:\n",
    "        y_true = [int(np.round(float(y))) for y in y_true]\n",
    "        y_pred = [int(np.round(float(y))) for y in y_pred]\n",
    "    except ValueError as e:\n",
    "        logger.error(\"For kappa, the labels should be integers or strings \"\n",
    "                     \"that can be converted to ints (E.g., '4.0' or '3').\")\n",
    "        raise e\n",
    "\n",
    "    # Figure out normalized expected values\n",
    "    min_rating = min(min(y_true), min(y_pred))\n",
    "    max_rating = max(max(y_true), max(y_pred))\n",
    "\n",
    "    # shift the values so that the lowest value is 0\n",
    "    # (to support scales that include negative values)\n",
    "    y_true = [y - min_rating for y in y_true]\n",
    "    y_pred = [y - min_rating for y in y_pred]\n",
    "\n",
    "    # Build the observed/confusion matrix\n",
    "    num_ratings = max_rating - min_rating + 1\n",
    "    observed = confusion_matrix(y_true, y_pred,\n",
    "                                labels=list(range(num_ratings)))\n",
    "    num_scored_items = float(len(y_true))\n",
    "\n",
    "    # Build weight array if weren't passed one\n",
    "    if isinstance(weights, string_types):\n",
    "        wt_scheme = weights\n",
    "        weights = None\n",
    "    else:\n",
    "        wt_scheme = ''\n",
    "    if weights is None:\n",
    "        weights = np.empty((num_ratings, num_ratings))\n",
    "        for i in range(num_ratings):\n",
    "            for j in range(num_ratings):\n",
    "                diff = abs(i - j)\n",
    "                if allow_off_by_one and diff:\n",
    "                    diff -= 1\n",
    "                if wt_scheme == 'linear':\n",
    "                    weights[i, j] = diff\n",
    "                elif wt_scheme == 'quadratic':\n",
    "                    weights[i, j] = diff ** 2\n",
    "                elif not wt_scheme:  # unweighted\n",
    "                    weights[i, j] = bool(diff)\n",
    "                else:\n",
    "                    raise ValueError('Invalid weight scheme specified for '\n",
    "                                     'kappa: {}'.format(wt_scheme))\n",
    "\n",
    "    hist_true = np.bincount(y_true, minlength=num_ratings)\n",
    "    hist_true = hist_true[: num_ratings] / num_scored_items\n",
    "    hist_pred = np.bincount(y_pred, minlength=num_ratings)\n",
    "    hist_pred = hist_pred[: num_ratings] / num_scored_items\n",
    "    expected = np.outer(hist_true, hist_pred)\n",
    "\n",
    "    # Normalize observed array\n",
    "    observed = observed / num_scored_items\n",
    "\n",
    "    # If all weights are zero, that means no disagreements matter.\n",
    "    k = 1.0\n",
    "    if np.count_nonzero(weights):\n",
    "        k -= (sum(sum(weights * observed)) / sum(sum(weights * expected)))\n",
    "\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lasso CV \n",
    "\n",
    "train_data = df_train_n.values.copy()\n",
    "test_data = df_test_n.values.copy()\n",
    "\n",
    "X_train = train_data[0:,2:]\n",
    "Y_train = train_data[0:,1]\n",
    "\n",
    "X_test = test_data[0:,2:]\n",
    "Y_test = test_data[0:,1]\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "clf = linear_model.LassoLarsCV()\n",
    "clf.fit(X_train, Y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "mms = preprocessing.MinMaxScaler()\n",
    "x = df[\"Response\"].values.astype(np.float)\n",
    "mms.fit_transform(x)\n",
    "\n",
    "pred_transformed = mms.inverse_transform(pred)\n",
    "Y_test_transformed = mms.inverse_transform(Y_test)\n",
    "\n",
    "k = kappa(pred_transformed, Y_test_transformed, weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters are:  1.04342989855e-07\n",
      "Kappa is:  0.358159226563\n"
     ]
    }
   ],
   "source": [
    "params = clf.alpha_\n",
    "\n",
    "print \"The parameters are: \", params\n",
    "print \"Kappa is: \", k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf_ak = pd.DataFrame(alpha_kappa,columns=[\"alpha\",\"kappa\"])   \\n\\nplt.figure(1, figsize=[10,10])\\nplt.subplot(211)\\nplt.title(\"alpha vs. kappa: linear lasso - test#1\")\\nplt.xlabel(\"alpha[0.001,0.1]\")\\nplt.ylabel(\"kappa\")\\nplt.legend\\nplt.scatter(x=df_ak.alpha,y=df_ak.kappa)\\n\\nplt.subplot(212)\\nplt.title(\"alpha vs. time: linear lasso - test#1\")\\nplt.xlabel(\"alpha[0.001,0.1]\")\\nplt.ylabel(\"time(s)\")\\nplt.legend\\nplt.scatter(x=df_ak.alpha,y=df_ak.time)\\n\\n#plt.savefig(\\'images/scatterLassoCV_alpha_kappa_test1.png\\')\\n    \\ndf_ak.describe()\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df_ak = pd.DataFrame(alpha_kappa,columns=[\"alpha\",\"kappa\"])   \n",
    "\n",
    "plt.figure(1, figsize=[10,10])\n",
    "plt.subplot(211)\n",
    "plt.title(\"alpha vs. kappa: linear lasso - test#1\")\n",
    "plt.xlabel(\"alpha[0.001,0.1]\")\n",
    "plt.ylabel(\"kappa\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ak.alpha,y=df_ak.kappa)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"alpha vs. time: linear lasso - test#1\")\n",
    "plt.xlabel(\"alpha[0.001,0.1]\")\n",
    "plt.ylabel(\"time(s)\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ak.alpha,y=df_ak.time)\n",
    "\n",
    "#plt.savefig('images/scatterLassoCV_alpha_kappa_test1.png')\n",
    "    \n",
    "df_ak.describe()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lasso model - test #1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data = df_train_n.values.copy()\n",
    "test_data = df_test_n.values.copy()\n",
    "\n",
    "X_train = train_data[0:,2:]\n",
    "Y_train = train_data[0:,1]\n",
    "\n",
    "X_test = test_data[0:,2:]\n",
    "Y_test = test_data[0:,1]\n",
    "\n",
    "alpha_kappa = list()\n",
    "\n",
    "for i in range(1,100,2):\n",
    "    \n",
    "    a = float(i)/1000\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=a)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    mms = preprocessing.MinMaxScaler()\n",
    "    x = df[\"Response\"].values.astype(np.float)\n",
    "    mms.fit_transform(x)\n",
    "\n",
    "    pred_transformed = mms.inverse_transform(pred)\n",
    "    Y_test_transformed = mms.inverse_transform(Y_test)\n",
    "    \n",
    "    k = kappa(pred_transformed, Y_test_transformed, weights='quadratic')\n",
    "    \n",
    "    alpha_kappa+=[[a,k,round(time()-t0,3)]]\n",
    "  \n",
    "  \n",
    "df_ak = pd.DataFrame(alpha_kappa,columns=[\"alpha\",\"kappa\",\"time\"])   \n",
    "\n",
    "plt.figure(1, figsize=[10,10])\n",
    "plt.subplot(211)\n",
    "plt.title(\"alpha vs. kappa: linear lasso - test#1\")\n",
    "plt.xlabel(\"alpha[0.001,0.1]\")\n",
    "plt.ylabel(\"kappa\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ak.alpha,y=df_ak.kappa)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"alpha vs. time: linear lasso - test#1\")\n",
    "plt.xlabel(\"alpha[0.001,0.1]\")\n",
    "plt.ylabel(\"time(s)\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ak.alpha,y=df_ak.time)\n",
    "\n",
    "plt.savefig('images/scatterLasso_alpha_kappa_test1.png')\n",
    "    \n",
    "df_ak.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Lasso  - Test #2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train_data = df_train_n.values.copy()\n",
    "test_data = df_test_n.values.copy()\n",
    "\n",
    "X_train = train_data[0:,2:]\n",
    "Y_train = train_data[0:,1]\n",
    "\n",
    "X_test = test_data[0:,2:]\n",
    "Y_test = test_data[0:,1]\n",
    "\n",
    "alpha_kappa = list()\n",
    "\n",
    "for i in range(1,100,2):\n",
    "    \n",
    "    a = float(i)/10000\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=a)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    mms = preprocessing.MinMaxScaler()\n",
    "    x = df[\"Response\"].values.astype(np.float)\n",
    "    mms.fit_transform(x)\n",
    "\n",
    "    pred_transformed = mms.inverse_transform(pred)\n",
    "    Y_test_transformed = mms.inverse_transform(Y_test)\n",
    "    \n",
    "    k = kappa(pred_transformed, Y_test_transformed, weights='quadratic')\n",
    "    \n",
    "    alpha_kappa+=[[a,k, round(time()-t0,3)]]\n",
    "  \n",
    "  \n",
    "df_ak = pd.DataFrame(alpha_kappa,columns=[\"alpha\",\"kappa\", \"time\"])    \n",
    "\n",
    "plt.figure(2, figsize=[10,10])\n",
    "plt.subplot(211)\n",
    "plt.title(\"alpha vs. kappa: linear lasso: test2\")\n",
    "plt.xlabel(\"alpha[0.0001,0.01]\")\n",
    "plt.ylabel(\"kappa\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ak.alpha,y=df_ak.kappa)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"alpha vs. time: linear lasso - test#2\")\n",
    "plt.xlabel(\"alpha[0.0001,0.01]\")\n",
    "plt.ylabel(\"time(s)\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ak.alpha,y=df_ak.time)\n",
    "\n",
    "plt.savefig('images/scatterLasso_alpha_kappa_test2.png')\n",
    "    \n",
    "df_ak.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Random forest - test # 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train_data = df_train_n.values.copy()\n",
    "test_data = df_test_n.values.copy()\n",
    "\n",
    "X_train = train_data[0:,2:]\n",
    "Y_train = train_data[0:,1]\n",
    "\n",
    "X_test = test_data[0:,2:]\n",
    "Y_test = test_data[0:,1]\n",
    "\n",
    "Y_train = np.array(Y_train).astype(int)\n",
    "\n",
    "est_kappa = list()\n",
    "\n",
    "for i in range(1,100,10):\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = i)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    mms = preprocessing.MinMaxScaler()\n",
    "    x = df[\"Response\"].values.astype(np.float)\n",
    "    mms.fit_transform(x)\n",
    "\n",
    "    pred_transformed = mms.inverse_transform(pred)\n",
    "    Y_test_transformed = mms.inverse_transform(Y_test)\n",
    "\n",
    "    k = kappa(pred_transformed, Y_test_transformed, weights='quadratic')\n",
    "    \n",
    "    est_kappa+=[[i,k,round(time()-t0,3)]]\n",
    "  \n",
    "  \n",
    "df_ek = pd.DataFrame(est_kappa,columns=[\"est\",\"kappa\",\"time\"])    \n",
    "\n",
    "plt.figure(3, figsize=[10,10])\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Estimators vs. kappa: RandomForest: Test1\")\n",
    "plt.xlabel(\"Estimators [1,100,10]\")\n",
    "plt.ylabel(\"kappa\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ek.est,y=df_ek.kappa)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Time vs. kappa: RandomForest: Test1\")\n",
    "plt.xlabel(\"Estimators [1,100,10]\")\n",
    "plt.ylabel(\"Time(s)\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ek.est,y=df_ek.time)\n",
    "\n",
    "plt.savefig('images/RFC_scatter_alpha_kappa_test1.png')\n",
    "    \n",
    "df_ek.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest - test # 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train_data = df_train_n.values.copy()\n",
    "test_data = df_test_n.values.copy()\n",
    "\n",
    "X_train = train_data[0:,2:]\n",
    "Y_train = train_data[0:,1]\n",
    "\n",
    "X_test = test_data[0:,2:]\n",
    "Y_test = test_data[0:,1]\n",
    "\n",
    "Y_train = np.array(Y_train).astype(int)\n",
    "\n",
    "est_kappa = list()\n",
    "\n",
    "for i in range(100,1000,100):\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = i)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    mms = preprocessing.MinMaxScaler()\n",
    "    x = df[\"Response\"].values.astype(np.float)\n",
    "    mms.fit_transform(x)\n",
    "\n",
    "    pred_transformed = mms.inverse_transform(pred)\n",
    "    Y_test_transformed = mms.inverse_transform(Y_test)\n",
    "\n",
    "    k = kappa(pred_transformed, Y_test_transformed, weights='quadratic')\n",
    "    \n",
    "    est_kappa+=[[i,k,round(time()-t0,3)]]\n",
    "  \n",
    "  \n",
    "df_ek = pd.DataFrame(est_kappa,columns=[\"est\",\"kappa\",\"time\"])    \n",
    "\n",
    "plt.figure(4, figsize=[10,10])\n",
    "\n",
    "plt.subplot(211)\n",
    "plt.title(\"Estimators vs. kappa: RandomForest: Test2\")\n",
    "plt.xlabel(\"Estimators [100,1000,100]\")\n",
    "plt.ylabel(\"kappa\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ek.est,y=df_ek.kappa)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title(\"Time vs. kappa: RandomForest: Test1\")\n",
    "plt.xlabel(\"Estimators [100,1000,100]\")\n",
    "plt.ylabel(\"Time(s)\")\n",
    "plt.legend\n",
    "plt.scatter(x=df_ek.est,y=df_ek.time)\n",
    "\n",
    "plt.savefig('images/RFC_scatter_alpha_kappa_test2.png')\n",
    "    \n",
    "df_ek.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Outputting file names in a folder\n",
    "\n",
    "from os import walk\n",
    "\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "    f.extend(filenames)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
