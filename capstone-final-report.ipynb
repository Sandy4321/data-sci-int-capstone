{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of life insurance data to support a data product proposal for an insurance brokerage\n",
    "\n",
    "### Author: Robbie Sharma (robbie.sharma@gmail.com)\n",
    "### Mentor: Hobson Lane (hobsonlane@gmail.com)\n",
    "### Prepared for: Springboard - Data Science Intensive Course \n",
    "### January 2016 - February 2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thanks\n",
    "\n",
    "I would like to thank Hobson Lane for his patience and support in this project.  In most of the 30 min mentor sessions, we were able to communicate well and I received a lot of useful information from him on the world of machine learning.  His knowledge, expertise and insane level of intelligence was very helpful in boosting my understanding of the concepts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Insurance brokers are challenged to retain their client base while growing business in diverse markets.  Personalizing insurance products and providing policies at competitive premiums are primary goals brokers have, although it is key for them to understand what factors can influence claims risk, the future continuance of policies, and the administrative overhead with managing their client policies.\n",
    "\n",
    "For the Data Science Intensive Capstone project, my goal was to understand data wrangling and predictive modelling by machine learning so I can help my insurance broker client meet or exceed their strategic retention and growth goals.  In order to achienve this, I analyzed a life insurance data set offered by Prudential Life Insurance through a competition on Kaggle.com.  By using data analytics and machine learning techniques,  I studied a structured and fairly \"clean\" data set in order to make predictions on the ordinal categorical risk rating based on profiles of life insurance customers.  The premise of this study is to show my client how insurance data can be used to identify and predict features and trends for policy holders.\n",
    "\n",
    "Considering the life insurance data set, methologies and analytical techniques are shown to identify interesting data features, normalize and transform certain features, and how to apply and tune simple machine learning algorithms for predictive purposes.   \n",
    "\n",
    "Multiple interviews were conducted with the CEO and CFO of the insurance brokerage in order to understand their business processes, data sources and types, and current management procedures to mitigate the risk of policy cancellations (retention), while expanding their client base.\n",
    "\n",
    "This report will make useful connections between solving a strucutred machine learning problem involving the life insurance dataset and how a similar methodology could be applied to the datasets from the insurance brokerage.\n",
    "\n",
    "\n",
    "\n",
    "## Problem description: Life insurance \n",
    "\n",
    "The goal of the life insurance Kaggle competition, is to solve a multi-classification problem by assigning a categorical (ordinal) risk rating  from 1-9 for a customer profile.  The algorithm accuracy is tested by measuring the quadratic weighted kappa (QWK), the inter-rater agreement between a predicted set to the actual set of risk ratings.  \n",
    "\n",
    "The data set contains 127 features (minus the ID feature) consisting of categorical (ordinal), continuous (normalized), and discrete feature types. There are 48 Medical Keyword features acting as dummy variables with binary, discrete values. \n",
    "\n",
    "The deliverables were a machine learning algorithm for predicting risk response in the Kaggle dataset. \n",
    "\n",
    "An appropriate process would need to be applied to transform the data with useful features selected, a machine learning algorithm implemented and the QWK calculated to determine error in classification.  The top competitors achieved QWK scores of 0.67939.\n",
    "\n",
    "## Problem description: Insurance brokerage\n",
    "\n",
    "My client is an insurance broker and has been in business for 21 years.  They have collected a significant amount of data on their clients, insurance agencies, products and sales representatives over the past 20 years. Due to confidentiality concerns, I cannot release any data although the strategies imployed will be discussed. \n",
    "\n",
    "By being able to find risk rating correlations between life insurance applicants, I can use the methods learned in this course and project to facilitate a discussion on how my client’s problem of improving retention can be resolved using similar methods.\n",
    "\n",
    "I explored their Applied Systems TAMS software (a insurance brokerage management tool).  The tool produces client proudct summaries, claims revenue, sales summaries, and accounting reports.  It can export into CSV. \n",
    "\n",
    "The deliverables were a memo outlining an approach to solving my client’s retention problem. and a machine learning algorithm for predicting risk response in the Kaggle dataset. A portion of the Email memo is attached in the Appendix.  \n",
    "\n",
    "In the context of this project, the goal is to determine what tasks would be useful in a data science project that could help my insurance broker client meet their company goals.  \n",
    "\n",
    "\n",
    "# Methods & Analysis\n",
    "\n",
    "The following methods and code was written were taken to analyze the data.  The primary goal was to compare Response ratings to the other features in the data set.  \n",
    "\n",
    "   1 Data exploration and plotting\n",
    "       1 The 48 Medical_Keyword_X columns were summed to derive a total medical keyword count feature.\n",
    "       1 Histogram plots of all the features were created to perform a preliminary exploration.\n",
    "       1 The following features were explored in this project in more detail: Ins_Age, Ht, Wt, BMI, Product_Info_2, and Medical_Keyword_Sum\n",
    "       1 Histogram plots and a scatter matrix plot were saved for the features listed above.\n",
    "   1 Data transformation and normalization\n",
    "       1 Replaced alpha-numeric labels in Product_Info_2 with an enumerated dictionary of dummy integers.  \n",
    "       1 Replaced all the NaNs in the dataset with -1.   \n",
    "       1 Normalization based on min max difference of data sets.  \n",
    "           1 Risk rating was normalized to values between 0 and 1.  \n",
    "           1 Product_Info_2 normalized between 0 and 1.\n",
    "           1 Categorical and discrete data sets were normalized to 0 and 1. Elements with a -1 or NaN were normalized to 0.\n",
    "   1 Machine learning\n",
    "       1 Training/Test set\n",
    "           1 10% of the train.csv data set used for a testing set and 90% used for a training set.        \n",
    "       1 Test and fitting using the following classifiers\n",
    "           *Linear Model - LASSO\n",
    "           *Random Forest\n",
    "       1 Evaluation\n",
    "           * Quadratic Weighted Kappa function from skll library used to classify the error\n",
    "           \n",
    "\n",
    "# Data exploration\n",
    "\n",
    "\n",
    "## Descriptive statistics\n",
    "\n",
    "There are 59380 rows in the dataset. Descriptive statistics are given in the following notebook: capstone-data-story-project.ipynb\n",
    "\n",
    "## Scatter matrix\n",
    "\n",
    "The following is observed in the scatter matrix:\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](images/scatter_matrix/Response_scatter_matrix_2016-03-06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "Histograms of selected features created at each response rating (1-8).  The following was observed:\n",
    "\n",
    "* \n",
    "\n",
    "### 'Ht' feature \n",
    "\n",
    "![caption](images/Ht/1_hist_Response_Ht_-2016-03-06.png)\n",
    "![caption](images/Ht/2_hist_Response_Ht_-2016-03-06.png)\n",
    "![caption](images/Ht/3_hist_Response_Ht_-2016-03-06.png)\n",
    "![caption](images/Ht/4_hist_Response_Ht_-2016-03-06.png)\n",
    "![caption](images/Ht/5_hist_Response_Ht_-2016-03-06.png)\n",
    "![caption](images/Ht/6_hist_Response_Ht_-2016-03-06.png)\n",
    "![caption](images/Ht/7_hist_Response_Ht_-2016-03-06.png)\n",
    "![caption](images/Ht/8_hist_Response_Ht_-2016-03-06.png)\n",
    "\n",
    "### 'BMI' feature \n",
    "\n",
    "\n",
    "![caption](images/BMI/1_hist_Response_BMI_-2016-03-06.png)\n",
    "![caption](images/BMI/2_hist_Response_BMI_-2016-03-06.png)\n",
    "![caption](images/BMI/3_hist_Response_BMI_-2016-03-06.png)\n",
    "![caption](images/BMI/4_hist_Response_BMI_-2016-03-06.png)\n",
    "![caption](images/BMI/5_hist_Response_BMI_-2016-03-06.png)\n",
    "![caption](images/BMI/6_hist_Response_BMI_-2016-03-06.png)\n",
    "![caption](images/BMI/7_hist_Response_BMI_-2016-03-06.png)\n",
    "![caption](images/BMI/8_hist_Response_BMI_-2016-03-06.png)\n",
    "\n",
    "### 'Ins_Age' feature \n",
    "![caption](images/Ins_Age/1_hist_Response_Ins_Age_-2016-03-06.png)\n",
    "![caption](images/Ins_Age/2_hist_Response_Ins_Age_-2016-03-06.png)\n",
    "![caption](images/Ins_Age/3_hist_Response_Ins_Age_-2016-03-06.png)\n",
    "![caption](images/Ins_Age/4_hist_Response_Ins_Age_-2016-03-06.png)\n",
    "![caption](images/Ins_Age/5_hist_Response_Ins_Age_-2016-03-06.png)\n",
    "![caption](images/Ins_Age/6_hist_Response_Ins_Age_-2016-03-06.png)\n",
    "![caption](images/Ins_Age/7_hist_Response_Ins_Age_-2016-03-06.png)\n",
    "![caption](images/Ins_Age/8_hist_Response_Ins_Age_-2016-03-06.png)\n",
    "\n",
    "### 'Product_Info_2' feature\n",
    "![caption](images/Product_Info_2/1_hist_Response_Product_Info_2_-2016-03-06.png)\n",
    "![caption](images/Product_Info_2/2_hist_Response_Product_Info_2_-2016-03-06.png)\n",
    "![caption](images/Product_Info_2/3_hist_Response_Product_Info_2_-2016-03-06.png)\n",
    "![caption](images/Product_Info_2/4_hist_Response_Product_Info_2_-2016-03-06.png)\n",
    "![caption](images/Product_Info_2/5_hist_Response_Product_Info_2_-2016-03-06.png)\n",
    "![caption](images/Product_Info_2/6_hist_Response_Product_Info_2_-2016-03-06.png)\n",
    "![caption](images/Product_Info_2/7_hist_Response_Product_Info_2_-2016-03-06.png)\n",
    "![caption](images/Product_Info_2/8_hist_Response_Product_Info_2_-2016-03-06.png)\n",
    "\n",
    "### 'Medical Keyword Sum' feature\n",
    "![caption](images/Medical_Keyword_Sum/1_hist_Response_Medical_Keyword_Sum_-2016-03-06.png)\n",
    "![caption](images/Medical_Keyword_Sum/2_hist_Response_Medical_Keyword_Sum_-2016-03-06.png)\n",
    "![caption](images/Medical_Keyword_Sum/3_hist_Response_Medical_Keyword_Sum_-2016-03-06.png)\n",
    "![caption](images/Medical_Keyword_Sum/4_hist_Response_Medical_Keyword_Sum_-2016-03-06.png)\n",
    "![caption](images/Medical_Keyword_Sum/5_hist_Response_Medical_Keyword_Sum_-2016-03-06.png)\n",
    "![caption](images/Medical_Keyword_Sum/6_hist_Response_Medical_Keyword_Sum_-2016-03-06.png)\n",
    "![caption](images/Medical_Keyword_Sum/7_hist_Response_Medical_Keyword_Sum_-2016-03-06.png)\n",
    "![caption](images/Medical_Keyword_Sum/8_hist_Response_Medical_Keyword_Sum_-2016-03-06.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Interesting graphs\n",
    "\n",
    "![caption](images/hist_Product_Info_2.png)\n",
    "\n",
    "-Shows exponental distribution of categories\n",
    "-Categories should be rearranged to make it normally distributed\n",
    "-D3 is very common occurance.... why?\n",
    "\n",
    "\n",
    "![caption](images/hist_product_info_4.png)\n",
    "-exponention distribution of categories... why?\n",
    "-can also be rearranged.\n",
    "\n",
    "![caption](images/hist_response.png)\n",
    "![caption](images/hist_norm_Response.png)\n",
    "\n",
    "\n",
    "## Other graphs\n",
    "![caption](images/hist_norm_Product_Info_1.png)\n",
    "![caption](images/hist_norm_Product_Info_3.png)\n",
    "![caption](images/hist_norm_Product_Info_5.png)\n",
    "![caption](images/hist_norm_Product_Info_6.png)\n",
    "![caption](images/hist_norm_Product_Info_7.png)\n",
    "\n",
    "\n",
    "![caption](images/hist_product_info_1.png)\n",
    "![caption](images/hist_Product_Info_3.png)\n",
    "![caption](images/hist_Product_Info_5.png)\n",
    "![caption](images/hist_Product_Info_6.png)\n",
    "![caption](images/hist_Product_Info_7.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Kaggle competition\n",
    "\n",
    "Highest value was 0.67939\n",
    "\n",
    "placed 2371/2619  - can't plug and play an algorithm to win Kaggle\n",
    "\n",
    "\n",
    "# Machine Learning Algorithms\n",
    "\n",
    "\n",
    "Lasso Linear modelling - measuring sparce coefficients.\n",
    "- Tuned with alpha value from 0 to 0.0099\n",
    "- A very low alpha suggests high variance and the algorithm is being over fitted. \n",
    "- Need to reduce the features to make the algorithm more generalized.\n",
    "\n",
    "Regularization reduces the overfitting problem\n",
    "\n",
    "L1 regularization - Lasso regression - a low alpha reduces the norm size of the input variables.  \n",
    "\n",
    "The alpha variable affects the regularization of the linear regression model.  \n",
    "\n",
    "(Favoured) L2 regularization - spreads out the shrinkage so all the interdependent variables are equally influential\n",
    "\n",
    "Random Forest Classifier.\n",
    "- Tuned with 0 to 1000 estimators\n",
    "\n",
    "\n",
    "## ML Results\n",
    "\n",
    "### alpha/time vs. kappa: linear lasso - test#1\n",
    "\n",
    "\talpha\tkappa\ttime\n",
    "count\t50.000000\t50.000000\t50.00000\n",
    "mean\t0.050000\t0.162239\t0.47140\n",
    "std\t0.029155\t0.086191\t0.09196\n",
    "min\t0.001000\t0.111002\t0.36900\n",
    "25%\t0.025500\t0.116683\t0.41125\n",
    "50%\t0.050000\t0.126014\t0.44200\n",
    "75%\t0.074500\t0.146227\t0.49200\n",
    "max\t0.099000\t0.461167\t0.79400\n",
    "\n",
    "![caption](images/scatterLasso_alpha_kappa_test1.png)\n",
    "\n",
    "# alpha/time vs. kappa: linear lasso: test2\n",
    "\n",
    "\talpha\tkappa\ttime\n",
    "count\t50.000000\t50.000000\t50.000000\n",
    "mean\t0.005000\t0.386544\t1.049080\n",
    "std\t0.002915\t0.049576\t2.596456\n",
    "min\t0.000100\t0.309687\t0.524000\n",
    "25%\t0.002550\t0.340325\t0.618250\n",
    "50%\t0.005000\t0.384313\t0.651500\n",
    "75%\t0.007450\t0.430685\t0.680500\n",
    "max\t0.009900\t0.473927\t19.014000\n",
    "\n",
    "![caption](images/scatterLasso_alpha_kappa_test2.png)\n",
    "\n",
    "# Estimators/Time vs. kappa: RandomForest: Test1\n",
    "\n",
    "\test\tkappa\ttime\n",
    "count\t10.000000\t10.000000\t10.000000\n",
    "mean\t46.000000\t0.337425\t19.335500\n",
    "std\t30.276504\t0.035419\t13.075482\n",
    "min\t1.000000\t0.259207\t0.530000\n",
    "25%\t23.500000\t0.325625\t9.757500\n",
    "50%\t46.000000\t0.346819\t18.733000\n",
    "75%\t68.500000\t0.358949\t28.620500\n",
    "max\t91.000000\t0.376106\t41.517000\n",
    "\n",
    "![caption](images/RFC_scatter_alpha_kappa_test1.png)\n",
    "\n",
    "\n",
    "# Estimators/Time vs. kappa: RandomForest: Test2\n",
    "\n",
    "\test\tkappa\ttime\n",
    "count\t9.000000\t9.000000\t9.000000\n",
    "mean\t500.000000\t0.357689\t210.043556\n",
    "std\t273.861279\t0.006708\t114.229644\n",
    "min\t100.000000\t0.346036\t42.440000\n",
    "25%\t300.000000\t0.357782\t123.817000\n",
    "50%\t500.000000\t0.358111\t214.093000\n",
    "75%\t700.000000\t0.362640\t291.163000\n",
    "max\t900.000000\t0.366685\t356.299000\n",
    "\n",
    "![caption](images/RFC_scatter_alpha_kappa_test2.png)\n",
    "\n",
    "\n",
    "## ML Evaluation criteria\n",
    "\n",
    "Accuracy_score, mean squared error and quadratic kappa were used. \n",
    "\n",
    "\n",
    "Algorithm placed 2300/2619\n",
    "\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "* Learned the difficulty in processing the data\n",
    "* Collected a lot of good code snippets to aid in future work\n",
    "* Linear lasso classifier had a higher QWK score although the alpha value of 0.0099 suggests the algorithm is over-fitting the data.  \n",
    "\n",
    "\n",
    "Lessons learned\n",
    "\n",
    "* when installing packages through a Windows-based Anaconda environment, use the 'conda install' command rather than 'pip install' to perform the package install.  I installed the 'skll' package using pip and my package environment was compromised, so I had to manually remove packages and reinstall many of them to work out the module error bugs in the code.  \n",
    "\n",
    "* To further improve the algorithm I would perform the following:\n",
    " * Split data set into 70% train and 30% test.\n",
    " * Select partial features to train the classifiers.  Product_Info_2, Product_Info_4, Ins_Agea and Medical_Keyword_Sum look like promising base features to include.  Employement_Info and Insure_History would need to be explored in more detail.\n",
    " * Use LassoCV to perform some further cross validation checks for linear regression.\n",
    " * Explore AdaBoost gradient \n",
    " \n",
    "\n",
    "## On the algorithm\n",
    "\n",
    "Tune the random forest.  Reduce the number of branches for faster processing, higher estimators\n",
    "\n",
    "Tune lasso with alpha of higher magnitude\n",
    "\n",
    "Create scatter plots of Response vs. each label\n",
    "\n",
    "Replace NaN with 0.  Disregard NaNs in data set\n",
    "\n",
    "\n",
    "## With the client\n",
    "\n",
    "* Give more visuals\n",
    "\n",
    "Reduce technical jargon\n",
    "\n",
    "Estimate work and present proposal fast\n",
    "\n",
    "\n",
    "# References\n",
    "\n",
    "http://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/\n",
    "\n",
    "http://scikit-learn.org/stable/modules/linear_model.html#lasso\n",
    "\n",
    "https://www.kaggle.com/c/prudential-life-insurance-assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Science Intensive: Capstone Milestone Report\n",
    "\n",
    "##Background\n",
    "\n",
    "I had decided to pursue a hybrid of two projects.  Analyzing life insurance applicant data for predicting risk rating.  Preparing a data analytics project proposal for an insurance brokerage.  \n",
    "\n",
    "The deliverables would be:\n",
    "\n",
    "* Prediction algorithms to determine risk rating for life insurance applicants.  \n",
    "* Project proposal for insurance brokerage data analytics project.\n",
    "\n",
    "\n",
    "###Life Insurance Deep Dive\n",
    "\n",
    "\n",
    "\n",
    " * Product Info\n",
    "   * Product_Info_1 has 2 categories.  Category, Rowcount =  [(1,57816) where row count = (2,1565)]\n",
    "   * Product_Info_2 has \n",
    "   * Product_Info_2 has \n",
    "   * Product_Info_2 has \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Insurance brokerage Deep Dive\n",
    "\n",
    "After observing the client's database I came to the following workflow and processes to assist my client in meeting their business goals.  \n",
    "\n",
    " 1.\tData cleanup/transformation.  \n",
    "    1.\tObserved duplicates, missing data, information not properly filled in etc.\n",
    "    1.\tNeed to investigate platform on how to perform mass changes and what is required to be changed\n",
    "    1.\tNeed to investigate if SQL database can be directly queried or there is an API to connect\n",
    " 2.  Data exploration\n",
    "    1.\tPerform ETL processes on TAM data using Python\n",
    "    1.\tIdentification of data types (continuous, discrete, categorical etc.)\n",
    "    1.\tIdentification of data features related to retention and cross-selling goals\n",
    " 3.\tData analytics\n",
    "    1.  Basic descriptive statistics on \n",
    "      1.  products\n",
    "      1.  representatives\n",
    "      1.  sales activities\n",
    "      1.  claims losses\n",
    "      1.  premium revenues\n",
    "    1. Basic Tables/charts -> top 20%, histograms, pie charts\n",
    "    1. Retention rates of different premium brackets\n",
    "       1.  New policies/Total policies, Lost policies/Total Policies  \n",
    "\t1. Customer segments (building 1st and 2nd  order models)\n",
    "       1,  Preimum brackets\n",
    "       1.  Combinations of meta data \n",
    "       1.  Income bracket, postal code, city, province, gender, age, personal? commercial? both?\n",
    "\t1. Discrete, continuous, and categorical time series signatures of “customer features”.  \n",
    "    1. Experimentation with machine learning and predictive models simple linear regression, SVM, decision trees  \n",
    " 4.\tData visualization\n",
    "    1.\tExcel, Qlikview, or Tableau dashboards… TBD after exploration and further needs assessments\n",
    " 5.\tManagement Consulting\n",
    "    1.\tRecommending reporting, decision-making and operating procedures/policies on retention and product cross-selling\n",
    "    1.\tIdentifying an appropriate reporting and analytics toolchain and workflow for the company "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Capstone milestone report questions\n",
    "\n",
    "1. An introduction to the problem: What is the problem? Who is the Client? (Feel free to reuse points 1-2 from your proposal document)\n",
    "1.  A deeper dive into the data set:\n",
    "  1. What important fields and information does the data set have?\t\n",
    "  1. What are its limitations i.e. what are some questions that you cannot answer with this data set?\n",
    "  1. What kind of cleaning and wrangling did you need to do?\n",
    "  1. Are there other datasets you can find, use and combine with, to answer the questions that matter?\n",
    "  1.  Any preliminary exploration you’ve performed and your initial findings. Test the hypotheses one at a time. Often, the data story emerges as a result of a sequence of testing hypothesis e.g. You first tested if X was true, and because it wasn't, you tried Y, which turned out to be true.\n",
    "  1.  Based on these findings, what approach are you going to take? How has your approach changed from what you initially proposed, if applicable?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CAPSTONE PROPOSAL - DISREGARD - USED AS reference\n",
    "\n",
    "Where I would perform machine learning analysis on an existing life insurance applicant data set, and take the learning patterns toThe problem I want to solve is to create a machine learning algorithm that predicts risk response based on a trained classifier set. A goal for their project would be to understand how retention rates differ for different product and customer segments and how best to improve the retention rates for the upcoming months.  Determining their high value clients and the ones most likely not to renew would form a significant part of their retention focused strategy.  \n",
    "\n",
    "The insurance client I have is interested in retention and growing their business, and wanted a better understanding of the data they had stored.  As a project, I wanted to analyze an insurance data set offered on Kaggle.com by Prudential Life Insurance.  The premise of the competition was to predict the risk response of a client based on a normalized dataset of current clients.  \n",
    "\n",
    "##Predicting risk rating for life insurance applicants - Capstone project\n",
    "\n",
    "The normalized dataset contains continuous data based on height, age, BMI.  Categorical (nominal) datasets based on risk response rating (1-7), medical histories, etc.  \n",
    "\n",
    "The main dependent variable is the Risk Response (1-8).\n",
    "\n",
    "### Project Goals\n",
    "The problem I want to solve is to create a machine learning algorithm that predicts risk response based on a trained classifier set.\n",
    "\n",
    "\n",
    "\n",
    "##Predicting retention risk for insurance brokerage - Brokerage project\n",
    "\n",
    "\n",
    "My client is an insurance broker and has been in business for 21 years.  They have collected a significant amount of data on their clients, insurance agencies, products and sales representatives over the past 20 years. Due to confidentiality concerns, I cannot release any data although the strategies imployed will be discussed. \n",
    "\n",
    "By being able to find risk rating correlations between life insurance applicants, I can use the methods learned in this course and project to facilitate a discussion on how my client’s problem of improving retention can be resolved using similar methods.\n",
    "\n",
    "I will be exploring their Applied Systems TAMS software (a insurance brokerage management tool).  The tool produces various reports they have and determining an action plan for a data analytics project.  This data can be exported into CSV format.\n",
    "The deliverables would be a memo outlining an approach to solving my client’s retention problem and a machine learning algorithm for predicting risk response in the Kaggle dataset. \n",
    "\n",
    "\n",
    "###Project Goals\n",
    "\n",
    "A goal for their project would be to understand how retention rates differ for different product and customer segments and how best to improve the retention rates for the upcoming months.  Determining their high value clients and the ones most likely not to renew would form a significant part of their retention focused strategy.  \n",
    "\n",
    "\n",
    "\n",
    "##Capstone project outline \n",
    "\n",
    "###Theme\n",
    "\n",
    "My capstone project will be a hybrid of two different projects.  The life insurance project will be to implement machine learning algorithms to predict risk rating.  The brokerage project will be to outline a proposal for a data analytics project based on my initial exploration study of their systems.  My reasoning is that there could be useful patterns in analyzing the life insurance data that could be useful in the brokerage project.  I will attempt to secure a paid work project based on what is learned in this course.\n",
    "\n",
    "###Data collection\n",
    "\n",
    "####Life insurance data set - Kaggle\n",
    "The Kaggle dataset is already fairly clean.  It does require separation and exploration of the data into different categories.\n",
    "\n",
    "####Insurance brokerage data - Applied Systems TAMS\n",
    "I need to interview the key executives at the company, particularly the controller, to understand what reports they use and how they use them to make decisions.  I also need an understanding how their data is collected and if there are any problems in data entry.  I need to present a data project proposal that can help meet their retention and growth requirements.  Information will also be collected on how their data can be exported, the most used reports, and figuring out a plan to extract, transform and load the datasets into something useful.\n",
    "\n",
    "###Deliverables\n",
    "\n",
    "  1.  Machine learning algorithm for predicting risk rating in life insurance applicants. iPython Notebook\n",
    "  2.  Project proposal for insurance brokerage on data analytics project improving retention rates and growth opportunities at  the firm. iPython Notebook\n",
    "  3.  Slide deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Capstone questions\n",
    "\n",
    "re is the evaluation criteria your mentor will use to grade the final submission:\n",
    "\n",
    "The technical quality of the work (50 points)\n",
    "Does the technical material make sense?\n",
    "Are the methods tried reasonable?\n",
    "Are the proposed algorithms or applications clever and interesting?\n",
    "\n",
    "Problem significance (15 points)\n",
    "Did the student choose an interesting or a “real” problem to work on, or only a small “toy” problem?\n",
    "Are the selected data sets for the problem “real”? Did the student do a good job with data acquisition, wrangling and cleaning?\n",
    "Is this work likely to be useful and/or have impact?\n",
    "\n",
    "Storytelling (25 points)\n",
    "Is there a clear, compelling, logical flow to the report and slide deck?\n",
    "Does it clearly motivate the problem, describe the solution and present the results as they’d be presented to the client?\n",
    "Is it clear what the client can do with the analysis - what is it that they can now do or decide differently than before the analysis?\n",
    "\n",
    "Deliverables (10 points)\n",
    "Did the student meet all the deadlines agreed to, and turn in all the material as expected (code on github, reports, slide decks etc)?\n",
    "Is the code well-documented?\n",
    "5 points extra credit: Did the student send the report or present it to the client? If the client did respond, how did they receive it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
